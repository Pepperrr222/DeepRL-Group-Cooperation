import torch
import torch.optim as optim
import numpy as np
import torch.nn.functional as F

from src.environment.game_env import NetworkGameEnv
from src.agents.human_bots import HumanBot
from src.planner.gnn_model import SocialPlannerAgent
from src.planner.policy import SocialPlannerPolicy

class SocialPlannerTrainer:
    def __init__(self, 
                 num_players=16, 
                 lr=1e-3, 
                 gamma=0.99, 
                 entropy_coef=0.01,
                 penalty_factor=0.5): # <--- [æ–°å¢] æƒ©ç½šå‚æ•° P (é»˜è®¤è®¾ä¸º 0.5)
        
        self.num_players = num_players
        self.gamma = gamma
        self.entropy_coef = entropy_coef
        self.penalty_factor = penalty_factor # å­˜å‚¨ P
        
        # 1. åˆå§‹åŒ–ä¸‰å¤§ä»¶
        self.env = NetworkGameEnv(num_players=num_players)
        self.bots = HumanBot(num_players=num_players)
        
        # 2. åˆå§‹åŒ– AI æ¨¡å‹
        self.planner = SocialPlannerAgent(input_node_dim=2, 
                                          input_edge_dim=1, 
                                          input_global_dim=1,
                                          hidden_dim=64)
        
        self.policy = SocialPlannerPolicy()
        
        # è®¾å¤‡ç®¡ç†
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        if self.device.type == 'cuda':
            print(f"ğŸš€ è®­ç»ƒè®¾å¤‡: GPU ({torch.cuda.get_device_name(0)})")
        else:
            print("âš ï¸ è®­ç»ƒè®¾å¤‡: CPU (æœªæ£€æµ‹åˆ°GPUï¼Œè¯·æ£€æŸ¥ç¯å¢ƒ)")

        self.planner.to(self.device)
        
        # 3. ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.planner.parameters(), lr=lr)

    def feature_adapter(self, adj_matrix, last_actions, payoffs, current_round, max_rounds):
        # ... (æ•°æ®è½¬æ¢éƒ¨åˆ†ä¿æŒä¸å˜) ...
        norm_payoffs = payoffs / (np.max(np.abs(payoffs)) + 1e-5) 
        node_feats = np.stack([last_actions, norm_payoffs], axis=1)
        x = torch.FloatTensor(node_feats).unsqueeze(0).to(self.device)
        edge_feats = adj_matrix.reshape(self.num_players, self.num_players, 1)
        edge_attr = torch.FloatTensor(edge_feats).unsqueeze(0).to(self.device)
        u = torch.FloatTensor([[current_round / max_rounds]]).to(self.device)
        return x, edge_attr, u

    def run_episode(self, max_rounds=15, train=True):
        self.env = NetworkGameEnv(self.num_players)
        self.bots = HumanBot(self.num_players)
        
        log_probs = []
        values = []
        rewards = []
        entropies = []
        
        current_payoffs = np.zeros(self.num_players)
        last_actions = np.zeros(self.num_players)
        
        total_cooperation_rate = 0
        
        for r in range(max_rounds):
            # --- Step 1-3: Planner æ€è€ƒä¸é‡‡æ · (ç•¥) ---
            x, edge_attr, u = self.feature_adapter(
                self.env.adj_matrix, last_actions, current_payoffs, r, max_rounds
            )
            
            edge_logits, value_est = self.planner(x, edge_attr, u)
            proposed_adj_tensor, log_prob, entropy = self.policy.get_action(edge_logits, deterministic=not train)
            proposed_adj = proposed_adj_tensor.squeeze(0).cpu().numpy()
            
            # --- Step 4: Bots å†³å®š & æ‹’ç»ç‡è¿½è¸ª [å…³é”®ä¿®æ”¹ç‚¹] ---
            current_adj = self.env.adj_matrix
            final_adj = current_adj.copy()
            
            # è¿½è¸ªå…¬å¼ (3) å’Œ (4) ä¸­çš„å˜é‡
            num_suggestions = 0  # m: æ€»å»ºè®®æ¬¡æ•° (Planner Action != 0)
            num_rejections = 0   # sum(f): è¢«æ‹’ç»çš„æ¬¡æ•° (Planner Action != 0 ä¸” Bots Action == 0)

            for i in range(self.num_players):
                for j in range(i + 1, self.num_players):
                    # Planner å»ºè®®äº†æ”¹å˜ (a_SP != 0)
                    if proposed_adj[i][j] != current_adj[i][j]:
                        num_suggestions += 1 
                        
                        action_type = 1 if proposed_adj[i][j] == 1 else -1 # 1=Make, -1=Break
                        
                        accept_i = self.bots.decide_acceptance(i, j, action_type, last_actions[j])
                        accept_j = self.bots.decide_acceptance(j, i, action_type, last_actions[i])
                        
                        # åŒæ–¹éƒ½åŒæ„æ‰ä¿®æ”¹ (Bots Action != 0)
                        if accept_i and accept_j:
                            final_adj[i][j] = final_adj[j][i] = proposed_adj[i][j]
                        else:
                            # Planner å»ºè®®äº†æ”¹å˜ï¼Œä½† Bots æ‹’ç»äº† (Bots Action == 0)
                            # æ»¡è¶³æƒ©ç½šæ¡ä»¶ f = 1
                            num_rejections += 1

            # --- Step 5: ç¯å¢ƒæ›´æ–° & æ¸¸æˆåšå¼ˆ (ç•¥) ---
            self.env.update_graph(final_adj)
            actions = self.bots.decide_cooperation(self.env.adj_matrix, current_round=r)
            step_payoffs = self.env.calculate_payoffs(actions)
            
            # --- Step 6: è®¡ç®—å¥–åŠ± (å…¬å¼ 3) [å…³é”®ä¿®æ”¹ç‚¹] ---
            
            # Term 1: å¹³å‡åˆä½œèµ„æœ¬ (1/n * sum(d_i))
            avg_payoff = np.mean(step_payoffs)
            
            # Term 2: æƒ©ç½šé¡¹ P * (1/m * sum(f))
            if num_suggestions > 0:
                # æƒ©ç½šé¡¹ = P * æ‹’ç»ç‡
                rejection_rate = num_rejections / num_suggestions
                penalty_term = self.penalty_factor * rejection_rate
            else:
                # æ²¡æœ‰å»ºè®®ï¼Œæ²¡æœ‰æƒ©ç½š
                penalty_term = 0
            
            # æœ€ç»ˆæ•ˆç”¨: U_sp = å¹³å‡æ”¶ç›Š - æƒ©ç½š
            step_reward = avg_payoff - penalty_term
            
            # å­˜å‚¨è½¨è¿¹
            log_probs.append(log_prob)
            values.append(value_est)
            rewards.append(step_reward) # å­˜å‚¨æ–°çš„å¥–åŠ±
            entropies.append(entropy)
            
            current_payoffs = step_payoffs
            last_actions = actions
            total_cooperation_rate += np.mean(actions)

        # --- Training Update (ç•¥) ---
        loss_value = 0
        if train:
            loss_value = self.update_model(rewards, values, log_probs, entropies)
            
        return {
            "mean_cooperation": total_cooperation_rate / max_rounds,
            "total_reward": np.sum(rewards),
            "loss": loss_value
        }

    # ... (update_model ä¿æŒä¸å˜ï¼Œå®ƒåªè´Ÿè´£ A2C æ¢¯åº¦è®¡ç®—) ...
    def update_model(self, rewards, values, log_probs, entropies):
        # 1. è®¡ç®—å›æŠ¥ (Returns)
        R = 0
        returns = []
        for r in rewards[::-1]:
            R = r + self.gamma * R
            returns.insert(0, R)
            
        returns = torch.tensor(returns, dtype=torch.float).to(self.device)
        values = torch.cat(values).squeeze(-1)
        log_probs = torch.cat(log_probs)
        entropies = torch.cat(entropies)
        
        # 2. è®¡ç®—ä¼˜åŠ¿ (Advantage)
        advantage = returns - values.detach()
        
        # 3. è®¡ç®— Loss
        actor_loss = -(log_probs * advantage).mean()
        critic_loss = F.mse_loss(values, returns)
        entropy_loss = -entropies.mean()
        
        total_loss = actor_loss + 0.5 * critic_loss + self.entropy_coef * entropy_loss
        
        # 4. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.planner.parameters(), 0.5)
        self.optimizer.step()
        
        return total_loss.item()