import matplotlib.pyplot as plt
import numpy as np
import torch
import os

# å¯¼å…¥æˆ‘ä»¬çš„è®­ç»ƒå™¨
from src.training.trainer import SocialPlannerTrainer

def main():
    # --- 1. é…ç½®å‚æ•° ---
    MAX_EPISODES = 10000   # è®­ç»ƒå¤šå°‘å±€ (è®ºæ–‡å¯èƒ½è®­ç»ƒäº†å‡ ä¸‡å±€ï¼Œæ¼”ç¤ºç”¨2000å³å¯çœ‹åˆ°æ•ˆæœ)
    PRINT_INTERVAL = 100   # æ¯éš”å¤šå°‘å±€æ‰“å°ä¸€æ¬¡æ—¥å¿—
    SAVE_PATH = "saved_models"
    os.makedirs(SAVE_PATH, exist_ok=True)
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Social Planner! ç›®æ ‡: {MAX_EPISODES} å±€")
    print("-" * 50)
    
    # --- 2. åˆå§‹åŒ–è®­ç»ƒå™¨ ---
    trainer = SocialPlannerTrainer(num_players=16, lr=0.001)
    
    # ç”¨äºè®°å½•æ•°æ®ç”»å›¾
    history = {
        "cooperation_rate": [],
        "reward": [],
        "loss": []
    }
    
    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for episode in range(1, MAX_EPISODES + 1):
        # è¿è¡Œä¸€æ•´å±€ (15 Rounds) å¹¶æ›´æ–°æ¨¡å‹
        metrics = trainer.run_episode(train=True)
        
        # è®°å½•æ•°æ®
        history["cooperation_rate"].append(metrics["mean_cooperation"])
        history["reward"].append(metrics["total_reward"])
        history["loss"].append(metrics["loss"])
        
        # æ‰“å°è¿›åº¦
        if episode % PRINT_INTERVAL == 0:
            # è®¡ç®—æœ€è¿‘50å±€çš„å¹³å‡å€¼ï¼Œæ•°æ®æ›´å¹³æ»‘
            avg_coop = np.mean(history["cooperation_rate"][-PRINT_INTERVAL:])
            avg_rew = np.mean(history["reward"][-PRINT_INTERVAL:])
            print(f"Episode {episode}/{MAX_EPISODES} | "
                  f"Coop Rate: {avg_coop:.2%} | "  # æ¯”å¦‚ 45.00%
                  f"Avg Reward: {avg_rew:.4f} | "
                  f"Loss: {metrics['loss']:.4f}")

    # --- 4. ä¿å­˜æ¨¡å‹ ---
    model_path = os.path.join(SAVE_PATH, "social_planner_final.pth")
    torch.save(trainer.planner.state_dict(), model_path)
    print("-" * 50)
    print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    # --- 5. å¯è§†åŒ–ç»“æœ ---
    plot_training_results(history)

def plot_training_results(history):
    """
    ç”»å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„åˆä½œç‡å’Œå¥–åŠ±å˜åŒ–æ›²çº¿
    """
    episodes = range(1, len(history["cooperation_rate"]) + 1)
    
    plt.figure(figsize=(12, 5))
    
    # å›¾1: åˆä½œç‡ (Cooperation Rate)
    plt.subplot(1, 2, 1)
    # ç»˜åˆ¶åŸå§‹æ•°æ® (åŠé€æ˜)
    plt.plot(episodes, history["cooperation_rate"], alpha=0.3, color='gray')
    # ç»˜åˆ¶ç§»åŠ¨å¹³å‡çº¿ (å¹³æ»‘)
    window_size = 50
    smooth_coop = np.convolve(history["cooperation_rate"], np.ones(window_size)/window_size, mode='valid')
    plt.plot(range(window_size, len(history["cooperation_rate"]) + 1), smooth_coop, color='blue', linewidth=2, label='Moving Avg')
    
    plt.title("Cooperation Rate over Time")
    plt.xlabel("Episode")
    plt.ylabel("Cooperation Rate (0-1)")
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # å›¾2: æ€»å¥–åŠ± (Total Reward)
    plt.subplot(1, 2, 2)
    plt.plot(episodes, history["reward"], alpha=0.3, color='gray')
    smooth_rew = np.convolve(history["reward"], np.ones(window_size)/window_size, mode='valid')
    plt.plot(range(window_size, len(history["reward"]) + 1), smooth_rew, color='orange', linewidth=2, label='Moving Avg')
    
    plt.title("Group Total Reward over Time")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig("training_curve.png")
    print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ä¸º: training_curve.png")
    # å¦‚æœæ˜¯åœ¨æœ¬åœ°è¿è¡Œï¼Œå¯ä»¥ç”¨ plt.show()
    # plt.show()

if __name__ == "__main__":
    main()