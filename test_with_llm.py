import torch
import numpy as np
import os
import matplotlib.pyplot as plt

from src.environment.game_env import NetworkGameEnv
from src.training.trainer import SocialPlannerTrainer
from src.agents.llm_bots import LLMBot

# ================= é…ç½®åŒºåŸŸ =================
MOCK_MODE = False  # <--- å…ˆè®¾ç½®ä¸º True è¿›è¡Œæµ‹è¯•ï¼è·‘é€šåå†æ”¹ä¸º False

# å½“ MOCK_MODE = False æ—¶ï¼Œä¸‹é¢è¿™äº›æ‰ç”Ÿæ•ˆ
API_KEY = "sk-aonzxraxsctwtfshddtbaytnqpikuwssvhendbhhizohiaol" 
BASE_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2.5-32B-Instruct"
# ===========================================

def main():
    print(f"ğŸš€ å¯åŠ¨å®éªŒ (æ¨¡å¼: {'Mock/æ¨¡æ‹Ÿ' if MOCK_MODE else 'Real/çœŸå®LLM'})")
    
    # 1. åŠ è½½ Planner
    trainer = SocialPlannerTrainer(num_players=16)
    model_path = "saved_models/social_planner_final.pth"
    
    if os.path.exists(model_path):
        trainer.planner.load_state_dict(torch.load(model_path))
        trainer.planner.eval()
        print(f"âœ… AI Planner æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ Planner è¿›è¡Œæ¼”ç¤ºã€‚")

    # 2. åˆå§‹åŒ–ç¯å¢ƒå’Œ Bots
    env = NetworkGameEnv(num_players=16)
    
    # æ³¨æ„è¿™é‡Œä¼ å…¥äº† mock å‚æ•°
    bots = LLMBot(num_players=16, 
                  api_key=API_KEY if not MOCK_MODE else "dummy", 
                  base_url=BASE_URL, 
                  model_name=MODEL_NAME,
                  mock=MOCK_MODE)
    
    # 3. è·‘ 3 è½®æµ‹è¯•
    MAX_ROUNDS = 10
    history_coop = []
    
    current_payoffs = np.zeros(16)
    last_actions = np.zeros(16)

    for r in range(MAX_ROUNDS):
        print(f"\n{'='*10} Round {r+1} {'='*10}")
        
        # --- A. Planner å»ºè®® ---
        x, edge_attr, u = trainer.feature_adapter(
            env.adj_matrix, last_actions, current_payoffs, r, MAX_ROUNDS
        )
        edge_logits, _ = trainer.planner(x, edge_attr, u)
        proposed_adj_tensor, _, _ = trainer.policy.get_action(edge_logits, deterministic=True)
        proposed_adj = proposed_adj_tensor.squeeze(0).cpu().detach().numpy()
        
        # --- B. Bot æ¥å—/æ‹’ç» ---
        current_adj = env.adj_matrix
        final_adj = current_adj.copy()
        changes = 0
        
        for i in range(16):
            for j in range(i + 1, 16):
                if proposed_adj[i][j] != current_adj[i][j]:
                    action_type = 1 if proposed_adj[i][j] == 1 else -1
                    if bots.decide_acceptance(i, j, action_type, last_actions[j]) and \
                       bots.decide_acceptance(j, i, action_type, last_actions[i]):
                        final_adj[i][j] = final_adj[j][i] = proposed_adj[i][j]
                        changes += 1
        
        print(f"âœ… ç½‘ç»œå˜åŠ¨: {changes} å¤„ä¿®æ”¹")
        env.update_graph(final_adj)
        
        # --- C. Bot å†³ç­– ---
        actions = bots.decide_cooperation(env.adj_matrix, r)
        
        # --- D. ç»Ÿè®¡ ---
        coop_rate = np.mean(actions)
        history_coop.append(coop_rate)
        current_payoffs = env.calculate_payoffs(actions)
        last_actions = actions
        
        print(f"ğŸ“Š åˆä½œç‡: {coop_rate:.2%}")

    # 4. ä¿å­˜ç»“æœå›¾
    plt.plot(range(1, MAX_ROUNDS+1), history_coop, marker='o')
    plt.title(f"Test Run (Mock={MOCK_MODE})")
    plt.savefig("test_llm_run.png")
    print("\nâœ… æµ‹è¯•é€šè¿‡ï¼ç»“æœå›¾å·²ä¿å­˜ä¸º test_llm_run.png")

if __name__ == "__main__":
    main()